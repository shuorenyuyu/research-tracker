# Research Tracker - System Architecture

## ðŸŽ¯ Design Philosophy

**One Quality Paper Per Day > 100 Unread Papers**

This system is designed around the principle that **curation beats volume** for investment research. Instead of overwhelming users with hundreds of papers, we deliver one carefully selected, high-citation paper daily from cutting-edge fields (AI, Robotics, New Energy, Biotechnology, Quantum Computing, etc.) with actionable insights.

## ðŸ—ï¸ Three-Phase Architecture

### Phase 1: Data Collection âœ…
**Status**: Complete  
**Frequency**: Daily at UTC 00:00  
**Source**: Semantic Scholar API

```
Keywords: ["artificial intelligence", "machine learning", "deep learning", "robotics", 
          "new energy", "battery technology", "solar energy", 
          "biotechnology", "gene editing", "synthetic biology",
          "quantum computing", "autonomous systems"]
         â†“
Semantic Scholar Search (top 100 by citations, year: current_year - 1 onwards)
         â†“
Deduplication Check (paper_id lookup)
         â†“
Store ONE New Paper (highest citations)
         â†“
SQLite Database (data/papers.db)
```

**Key Design Decisions**:
- **Semantic Scholar over arXiv**: Built-in citation data (no extra enrichment needed)
- **Citation ranking over recency**: Community validation matters more than novelty
- **Recent one year window**: Papers from (current_year - 1) onwards have time to accumulate meaningful citations
- **One paper per day**: Prevents database bloat, ensures each paper gets attention
- **Deduplication by paper_id**: Never fetch same paper twice

**Rate Limits**:
- Semantic Scholar: 100 requests / 5 minutes (free tier)
- Current usage: ~4 requests per run (one per keyword)
- Delays: 3 seconds between requests

### Phase 2: AI Summarization ðŸš§
**Status**: In Progress  
**Frequency**: Daily after Phase 1  
**Engine**: Azure OpenAI (GPT-4)

```
Unprocessed Papers (processed=False)
         â†“
Azure OpenAI API Call #1: Chinese Summary (300-500 chars)
         â†“
Azure OpenAI API Call #2: Investment Insights (200-400 chars)
         â†“
Update Database (summary_zh, investment_insights, processed=True)
```

**Prompt Engineering**:

**Summary Prompt**:
```
ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„AIå’Œæœºå™¨äººé¢†åŸŸç ”ç©¶åˆ†æžå¸ˆï¼Œæ“…é•¿ç”¨ä¸­æ–‡æ€»ç»“å­¦æœ¯è®ºæ–‡çš„æ ¸å¿ƒå†…å®¹ã€‚

è¯·åŒ…æ‹¬ï¼š
1. ç ”ç©¶èƒŒæ™¯å’ŒåŠ¨æœº
2. ä¸»è¦æ–¹æ³•/æŠ€æœ¯
3. æ ¸å¿ƒè´¡çŒ®å’Œåˆ›æ–°ç‚¹
4. å®žéªŒç»“æžœï¼ˆå¦‚æœ‰ï¼‰
5. æ½œåœ¨åº”ç”¨åœºæ™¯
```

**Insights Prompt**:
```
åŸºäºŽä»¥ä¸‹AI/æœºå™¨äººé¢†åŸŸçš„å­¦æœ¯è®ºæ–‡ï¼Œåˆ†æžå…¶æŠ•èµ„ä»·å€¼å’ŒæŠ€æœ¯è¶‹åŠ¿ã€‚

è¯·ä»ŽæŠ•èµ„è§’åº¦åˆ†æžï¼š
1. æŠ€æœ¯æˆç†Ÿåº¦ï¼ˆæ—©æœŸç ”ç©¶ vs åº”ç”¨å°±ç»ªï¼‰
2. å•†ä¸šåŒ–æ½œåŠ›ï¼ˆå¯èƒ½çš„äº§å“/æœåŠ¡æ–¹å‘ï¼‰
3. ç›¸å…³è¡Œä¸š/å…¬å¸ï¼ˆå¯èƒ½å—ç›Šçš„é¢†åŸŸï¼‰
4. æŠ•èµ„å»ºè®®ï¼ˆå…³æ³¨ç‚¹/é£Žé™©æç¤ºï¼‰
```

**Token Economics**:
- Summary: ~800 tokens (prompt) + ~300 tokens (response)
- Insights: ~600 tokens (prompt) + ~200 tokens (response)
- **Total per paper**: ~1,900 tokens (~$0.01 USD with GPT-4)
- **Monthly cost**: ~$0.30 USD (30 papers)

### Phase 3: Article Export âœ…
**Status**: Complete  
**Frequency**: On-demand or daily after Phase 2  
**Destination**: WeChat-formatted Markdown/HTML

```
Processed Papers (processed=True)
         â†“
Generate WeChat Article (Markdown + HTML)
         â†“
Save to data/wechat_articles/
         â†“
Manual copy/paste to WeChat å…¬ä¼—å· editor
```

**Article Format** (WeChat):
```markdown
# ðŸ”¬ ä»Šæ—¥AIå‰æ²¿è®ºæ–‡è§£è¯»

## ðŸ“„ è®ºæ–‡ä¿¡æ¯
æ ‡é¢˜: [Paper Title]
ä½œè€…: [Authors]
å‘è¡¨: [Venue] ([Year])
å¼•ç”¨æ•°: [Citation Count] æ¬¡

## ðŸ“– æ·±åº¦è§£è¯»
[summary_zh with 5 structured sections]

## ðŸ’° æŠ•èµ„è§†è§’
[investment_insights]

## ðŸ“Œ åŽŸæ–‡æ‘˜è¦
[abstract]

> ðŸ’¡ å…³äºŽæœ¬ç³»åˆ—
> æ¯æ—¥ç²¾é€‰ä¸€ç¯‡é«˜å¼•ç”¨AI/æœºå™¨äººé¢†åŸŸè®ºæ–‡...
```

## ðŸ—„ï¸ Database Schema

**Table**: `papers`

| Column | Type | Description | Example |
|--------|------|-------------|---------|
| `id` | INTEGER | Primary key | 1 |
| `paper_id` | VARCHAR(200) | **Unique** identifier from source | `"CorpusID:265213145"` |
| `source` | VARCHAR(50) | Data source | `"semantic_scholar"` |
| `title` | TEXT | Paper title | `"Attention Is All You Need"` |
| `authors` | TEXT | Comma-separated | `"Vaswani, A., Shazeer, N."` |
| `year` | INTEGER | Publication year | `2017` |
| `venue` | VARCHAR(200) | Conference/Journal | `"NeurIPS"` |
| `abstract` | TEXT | Full abstract | `"The dominant..."` |
| `url` | VARCHAR(500) | Semantic Scholar URL | `"https://..."` |
| `pdf_url` | VARCHAR(500) | Direct PDF link | `"https://arxiv.org/pdf/..."` |
| `doi` | VARCHAR(200) | DOI identifier | `"10.48550/arXiv.1706.03762"` |
| `citation_count` | INTEGER | Current citations | `114523` |
| `summary_zh` | TEXT | **Chinese summary** | `"æœ¬æ–‡æå‡º..."` |
| `investment_insights` | TEXT | **Investment analysis** | `"æŠ€æœ¯æˆç†Ÿåº¦..."` |
| `keywords` | VARCHAR(500) | Search keywords used | `"deep learning"` |
| `fetched_at` | DATETIME | When scraped | `"2025-12-07 00:00:05"` |
| `processed` | BOOLEAN | AI summarized? | `False` â†’ `True` |
| `published` | BOOLEAN | Exported to article? | `False` â†’ `True` |

**Indexes**:
- `paper_id` (UNIQUE) - Fast deduplication lookup
- `processed, fetched_at` - Query unprocessed papers

## ðŸ“ Code Structure

```
src/
â”œâ”€â”€ config/
â”‚   â””â”€â”€ settings.py              # Environment variables, paths
â”‚
â”œâ”€â”€ database/
â”‚   â”œâ”€â”€ models.py                # SQLAlchemy Paper model
â”‚   â””â”€â”€ repository.py            # CRUD operations, queries
â”‚
â”œâ”€â”€ scrapers/
â”‚   â”œâ”€â”€ arxiv_scraper.py         # Legacy (not used)
â”‚   â””â”€â”€ semantic_scholar_scraper.py  # Primary data source
â”‚
â”œâ”€â”€ processors/
â”‚   â””â”€â”€ azure_summarizer.py      # Azure OpenAI client
â”‚
â””â”€â”€ scheduler/
    â”œâ”€â”€ daily_scheduler.py       # Phase 1: Fetch papers
    â””â”€â”€ process_papers.py        # Phase 2: Summarize with Azure
```

## ðŸ”„ Daily Workflow

### Automated Schedule (macOS LaunchAgent)

**Trigger**: `com.researchtracker.scheduler.plist` (UTC 00:00)

```bash
# Phase 1: Fetch (00:00:00 - 00:00:15)
python3 src/scheduler/daily_scheduler.py --run-once
  â†’ Fetches top 100 papers from Semantic Scholar
  â†’ Checks for duplicates
  â†’ Adds ONE new paper (highest citations)
  â†’ Logs: "Today's paper added: 1"

# Phase 2: Summarize (00:00:15 - 00:00:45)
python3 src/scheduler/process_papers.py --one
  â†’ Finds unprocessed paper
  â†’ Generates Chinese summary (Azure OpenAI)
  â†’ Generates investment insights (Azure OpenAI)
  â†’ Marks processed=True
  â†’ Logs: "âœ… Successfully processed: [title]"

# Phase 3: Export (00:00:45 - 00:00:50) [On-demand]
python3 scripts/generate_wechat_article.py
  â†’ Finds latest processed paper
  â†’ Generates WeChat-formatted article
  â†’ Saves Markdown + HTML to data/wechat_articles/
  â†’ Ready for manual copy/paste to WeChat
```

**Total Runtime**: ~60 seconds  
**Network Calls**: 6-8 (4 Semantic Scholar + 2 Azure OpenAI)

## ðŸ›¡ï¸ Error Handling

### Semantic Scholar Rate Limiting
```python
try:
    response = requests.get(url)
    if response.status_code == 429:
        logger.warning("Rate limit hit, waiting 60s...")
        time.sleep(60)
        response = requests.get(url)  # Retry once
except Exception as e:
    logger.error(f"Semantic Scholar error: {e}")
    return []  # Graceful degradation
```

### Azure OpenAI Failures
```python
try:
    summary = summarizer.generate_summary(paper)
except Exception as e:
    logger.error(f"Azure OpenAI error: {e}")
    summary = None  # Don't mark as processed
    # Will retry tomorrow
```

### Deduplication Edge Cases
- **Same paper, different IDs**: Prevented by using Semantic Scholar's canonical `paper_id`
- **No new papers**: Logs "No new papers found" but doesn't crash
- **Database corruption**: SQLite auto-recovers, backup in `data/papers.db.backup`

## ðŸ” Security & Configuration

**Environment Variables** (`.env`):
```bash
# Azure OpenAI (required for Phase 2)
AZURE_OPENAI_ENDPOINT=https://xxx.openai.azure.com/
AZURE_OPENAI_API_KEY=sk-xxxxxxxxxxxxx
AZURE_OPENAI_DEPLOYMENT_NAME=gpt-4
AZURE_OPENAI_API_VERSION=2024-02-15-preview

# Database (default: SQLite)
DATABASE_URL=sqlite:///data/papers.db
```

**Secrets Management**:
- `.env` in `.gitignore` (never committed)
- `.env.example` provides template
- Production: Use macOS Keychain or Azure Key Vault

## ðŸ“Š Monitoring & Logs

**Log Files** (`data/logs/`):
- `scheduler.log` - Daily fetch operations
- `processor.log` - Azure OpenAI summaries

**Key Metrics**:
```bash
# Check today's fetch
tail -20 data/logs/scheduler.log

# Count total papers
sqlite3 data/papers.db "SELECT COUNT(*) FROM papers;"

# Count unprocessed
sqlite3 data/papers.db "SELECT COUNT(*) FROM papers WHERE processed=0;"

# Top cited papers
sqlite3 data/papers.db "SELECT title, citation_count FROM papers ORDER BY citation_count DESC LIMIT 10;"
```

## ðŸš€ Deployment Options

### Option 1: macOS LaunchAgent (Recommended)
```bash
./deployment/manage_scheduler.sh install  # Runs at boot
./deployment/manage_scheduler.sh status   # Check if running
./deployment/manage_scheduler.sh logs     # View output
```

**Pros**: Always running, survives reboots  
**Cons**: macOS only

### Option 2: Docker Container
```bash
docker build -t research-tracker .
docker run -d --env-file .env research-tracker
```

**Pros**: Platform-independent, isolated  
**Cons**: Needs Docker setup

### Option 3: Cloud Function (Future)
- AWS Lambda / Azure Functions
- Triggered by CloudWatch/EventGrid cron
- **Pros**: Serverless, scalable
- **Cons**: Cold starts, vendor lock-in

## ðŸ§ª Testing Strategy

**Unit Tests** (`tests/`):
```bash
pytest tests/test_repository.py      # Database operations
pytest tests/test_scraper.py         # Semantic Scholar API
pytest tests/test_summarizer.py      # Azure OpenAI (mocked)
```

**Integration Tests**:
```bash
# End-to-end daily workflow
python3 src/scheduler/daily_scheduler.py --run-once
python3 src/scheduler/process_papers.py --one
python3 scripts/show_papers.py  # Verify results
```

**Manual QA Checklist**:
- [ ] Deduplication works (no duplicate papers)
- [ ] Citation count > 0 (Semantic Scholar data quality)
- [ ] Chinese summary readable (Azure prompt quality)
- [ ] Investment insights actionable (prompt engineering)
- [ ] Logs show no errors (error handling works)

## ðŸ”® Future Enhancements

1. **Multi-Language Support**: English summaries for international investors
2. **Topic Clustering**: Group papers by research area (NLP, CV, RL, etc.)
3. **Citation Velocity**: Track how fast citations grow (hot topics)
4. **Author Networks**: Identify influential research groups
5. **Company Mentions**: Extract startup/company references from papers
6. **Weekly Digest**: Roll up 7 papers into trend analysis
7. **User Feedback Loop**: Allow rating papers to improve selection

## ðŸ“š References

- [Semantic Scholar API Docs](https://api.semanticscholar.org/)
- [Azure OpenAI Quickstart](https://learn.microsoft.com/en-us/azure/ai-services/openai/quickstart)
- [APScheduler Documentation](https://apscheduler.readthedocs.io/)

---

**Last Updated**: December 7, 2025  
**Version**: 2.0 (One Paper Per Day)
