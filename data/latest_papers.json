[
  {
    "title": "The Universal Weight Subspace Hypothesis",
    "paper_id": "2512.05117v1",
    "source": "arxiv",
    "authors": "Prakhar Kaushik, Shravan Chaudhari, Ankit Vaidya, Rama Chellappa, Alan Yuille",
    "first_author": "Prakhar Kaushik",
    "year": 2025,
    "publication_date": "2025-12-04 18:59:58+00:00",
    "venue": "cs.LG",
    "publisher": "arXiv",
    "abstract": "We show that deep neural networks trained across diverse tasks exhibit remarkably similar low-dimensional parametric subspaces. We provide the first large-scale empirical evidence that demonstrates that neural networks systematically converge to shared spectral subspaces regardless of initialization, task, or domain. Through mode-wise spectral analysis of over 1100 models - including 500 Mistral-7B LoRAs, 500 Vision Transformers, and 50 LLaMA-8B models - we identify universal subspaces capturing majority variance in just a few principal directions. By applying spectral decomposition techniques to the weight matrices of various architectures trained on a wide range of tasks and datasets, we identify sparse, joint subspaces that are consistently exploited, within shared architectures across diverse tasks and datasets. Our findings offer new insights into the intrinsic organization of information within deep networks and raise important questions about the possibility of discovering these universal subspaces without the need for extensive data and computational resources. Furthermore, this inherent structure has significant implications for model reusability, multi-task learning, model merging, and the development of training and inference-efficient algorithms, potentially reducing the carbon footprint of large-scale neural models.",
    "url": "http://arxiv.org/abs/2512.05117v1",
    "pdf_url": "https://arxiv.org/pdf/2512.05117v1",
    "citation_count": 0,
    "fetched_at": "2025-12-07 09:56:09.397874"
  },
  {
    "title": "DraCo: Draft as CoT for Text-to-Image Preview and Rare Concept Generation",
    "paper_id": "2512.05112v1",
    "source": "arxiv",
    "authors": "Dongzhi Jiang, Renrui Zhang, Haodong Li, Zhuofan Zong, Ziyu Guo, Jun He, Claire Guo, Junyan Ye, Rongyao Fang, Weijia Li, Rui Liu, Hongsheng Li",
    "first_author": "Dongzhi Jiang",
    "year": 2025,
    "publication_date": "2025-12-04 18:59:53+00:00",
    "venue": "cs.CV",
    "publisher": "arXiv",
    "abstract": "Recent unified multimodal large language models (MLLMs) have shown impressive capabilities, incorporating chain-of-thought (CoT) reasoning for enhanced text-to-image generation. However, existing approaches remain limited, either treating the model merely as a standalone generator or relying on abstract textual planning. To this end, we propose Draft-as-CoT (DraCo), a novel interleaved reasoning paradigm that fully leverages both textual and visual contents in CoT for better planning and verification. Our method first generates a low-resolution draft image as preview, providing more concrete and structural visual planning and guidance. Then, we employ the model's inherent understanding capability to verify potential semantic misalignments between the draft and input prompt, and performs refinement through selective corrections with super-resolution. In this way, our approach addresses two fundamental challenges: the coarse-grained nature of textual planning and the difficulty in generating rare attribute combinations. To support training, we curate DraCo-240K, aiming to enhance three atomic capabilities spanning general correction, instance manipulation, and layout reorganization. Supported by DraCo-CFG, a specialized classifier-free guidance (CFG) strategy for interleaved reasoning, DraCo achieves a tremendous increase on GenEval (+8%), Imagine-Bench (+0.91), and GenEval++ (+3%), significantly outperforming direct generation and other generation methods empowered by CoT.",
    "url": "http://arxiv.org/abs/2512.05112v1",
    "pdf_url": "https://arxiv.org/pdf/2512.05112v1",
    "citation_count": 0,
    "fetched_at": "2025-12-07 09:56:09.397914"
  },
  {
    "title": "ShadowDraw: From Any Object to Shadow-Drawing Compositional Art",
    "paper_id": "2512.05110v1",
    "source": "arxiv",
    "authors": "Rundong Luo, Noah Snavely, Wei-Chiu Ma",
    "first_author": "Rundong Luo",
    "year": 2025,
    "publication_date": "2025-12-04 18:59:51+00:00",
    "venue": "cs.CV",
    "publisher": "arXiv",
    "abstract": "We introduce ShadowDraw, a framework that transforms ordinary 3D objects into shadow-drawing compositional art. Given a 3D object, our system predicts scene parameters, including object pose and lighting, together with a partial line drawing, such that the cast shadow completes the drawing into a recognizable image. To this end, we optimize scene configurations to reveal meaningful shadows, employ shadow strokes to guide line drawing generation, and adopt automatic evaluation to enforce shadow-drawing coherence and visual quality. Experiments show that ShadowDraw produces compelling results across diverse inputs, from real-world scans and curated datasets to generative assets, and naturally extends to multi-object scenes, animations, and physical deployments. Our work provides a practical pipeline for creating shadow-drawing art and broadens the design space of computational visual art, bridging the gap between algorithmic design and artistic storytelling. Check out our project page https://red-fairy.github.io/ShadowDraw/ for more results and an end-to-end real-world demonstration of our pipeline!",
    "url": "http://arxiv.org/abs/2512.05110v1",
    "pdf_url": "https://arxiv.org/pdf/2512.05110v1",
    "citation_count": 0,
    "fetched_at": "2025-12-07 09:56:09.397940"
  },
  {
    "title": "Semantic Soft Bootstrapping: Long Context Reasoning in LLMs without Reinforcement Learning",
    "paper_id": "2512.05105v1",
    "source": "arxiv",
    "authors": "Purbesh Mitra, Sennur Ulukus",
    "first_author": "Purbesh Mitra",
    "year": 2025,
    "publication_date": "2025-12-04 18:59:18+00:00",
    "venue": "cs.CL",
    "publisher": "arXiv",
    "abstract": "Long context reasoning in large language models (LLMs) has demonstrated enhancement of their cognitive capabilities via chain-of-thought (CoT) inference. Training such models is usually done via reinforcement learning with verifiable rewards (RLVR) in reasoning based problems, like math and programming. However, RLVR is limited by several bottlenecks, such as, lack of dense reward, and inadequate sample efficiency. As a result, it requires significant compute resources in post-training phase. To overcome these limitations, in this work, we propose \\textbf{Semantic Soft Bootstrapping (SSB)}, a self-distillation technique, in which the same base language model plays the role of both teacher and student, but receives different semantic contexts about the correctness of its outcome at training time. The model is first prompted with a math problem and several rollouts are generated. From them, the correct and most common incorrect response are filtered, and then provided to the model in context to produce a more robust, step-by-step explanation with a verified final answer. This pipeline automatically curates a paired teacher-student training set from raw problem-answer data, without any human intervention. This generation process also produces a sequence of logits, which is what the student model tries to match in the training phase just from the bare question alone. In our experiment, Qwen2.5-3B-Instruct on GSM8K dataset via parameter-efficient fine-tuning. We then tested its accuracy on MATH500, and AIME2024 benchmarks. Our experiments show a jump of 10.6%, and 10% improvements in accuracy, respectively, over group relative policy optimization (GRPO), which is a commonly used RLVR algorithm. Our code is available at https://github.com/purbeshmitra/semantic-soft-bootstrapping, and the model, curated dataset is available at https://huggingface.co/purbeshmitra/semantic-soft-bootstrapping.",
    "url": "http://arxiv.org/abs/2512.05105v1",
    "pdf_url": "https://arxiv.org/pdf/2512.05105v1",
    "citation_count": 0,
    "fetched_at": "2025-12-07 09:56:09.397969"
  },
  {
    "title": "TV2TV: A Unified Framework for Interleaved Language and Video Generation",
    "paper_id": "2512.05103v1",
    "source": "arxiv",
    "authors": "Xiaochuang Han, Youssef Emad, Melissa Hall, John Nguyen, Karthik Padthe, Liam Robbins, Amir Bar, Delong Chen, Michal Drozdzal, Maha Elbayad, Yushi Hu, Shang-Wen Li, Sreya Dutta Roy, Jakob Verbeek, XuDong Wang, Marjan Ghazvininejad, Luke Zettlemoyer, Emily Dinan",
    "first_author": "Xiaochuang Han",
    "year": 2025,
    "publication_date": "2025-12-04 18:59:09+00:00",
    "venue": "cs.LG",
    "publisher": "arXiv",
    "abstract": "Video generation models are rapidly advancing, but can still struggle with complex video outputs that require significant semantic branching or repeated high-level reasoning about what should happen next. In this paper, we introduce a new class of omni video-text models that integrate ideas from recent LM reasoning advances to address this challenge. More specifically, we present TV2TV, a unified generative modeling framework which decomposes video generation into an interleaved text and video generation process. TV2TV jointly learns language modeling (next-token prediction) and video flow matching (next-frame prediction) using a Mixture-of-Transformers (MoT) architecture. At inference time, TV2TV decides when to alternate between generating text and video frames, allowing the model to \"think in words\" about subsequent content before ``acting in pixels'' to produce frames. This design offloads much of the responsibility for deciding what should happen next to the language modeling tower, enabling improved visual quality and prompt alignment of generated videos. It also enables fine-grained controllability, allowing users to modify the video generation trajectory through text interventions at any point in the process. In controlled experiments on video game data, TV2TV demonstrates substantial improvements in both visual quality and controllability. TV2TV also scales to natural videos, as we show by augmenting sports videos with interleaved natural language action descriptions using vision-language models (VLMs). Training TV2TV on this corpus yields strong visual quality and prompt alignment, showcasing the model's ability to reason about and generate complex real-world action sequences. Together, these results highlight TV2TV as a promising step toward video generation with open-ended textual reasoning and control.",
    "url": "http://arxiv.org/abs/2512.05103v1",
    "pdf_url": "https://arxiv.org/pdf/2512.05103v1",
    "citation_count": 0,
    "fetched_at": "2025-12-07 09:56:09.398007"
  },
  {
    "title": "Structured Document Translation via Format Reinforcement Learning",
    "paper_id": "2512.05100v1",
    "source": "arxiv",
    "authors": "Haiyue Song, Johannes Eschbach-Dymanus, Hour Kaing, Sumire Honda, Hideki Tanaka, Bianka Buschbeck, Masao Utiyama",
    "first_author": "Haiyue Song",
    "year": 2025,
    "publication_date": "2025-12-04 18:58:30+00:00",
    "venue": "cs.CL",
    "publisher": "arXiv",
    "abstract": "Recent works on structured text translation remain limited to the sentence level, as they struggle to effectively handle the complex document-level XML or HTML structures. To address this, we propose \\textbf{Format Reinforcement Learning (FormatRL)}, which employs Group Relative Policy Optimization on top of a supervised fine-tuning model to directly optimize novel structure-aware rewards: 1) TreeSim, which measures structural similarity between predicted and reference XML trees and 2) Node-chrF, which measures translation quality at the level of XML nodes. Additionally, we apply StrucAUC, a fine-grained metric distinguishing between minor errors and major structural failures. Experiments on the SAP software-documentation benchmark demonstrate improvements across six metrics and an analysis further shows how different reward functions contribute to improvements in both structural and translation quality.",
    "url": "http://arxiv.org/abs/2512.05100v1",
    "pdf_url": "https://arxiv.org/pdf/2512.05100v1",
    "citation_count": 0,
    "fetched_at": "2025-12-07 09:56:09.398033"
  },
  {
    "title": "SA-IQA: Redefining Image Quality Assessment for Spatial Aesthetics with Multi-Dimensional Rewards",
    "paper_id": "2512.05098v1",
    "source": "arxiv",
    "authors": "Yuan Gao, Jin Song",
    "first_author": "Yuan Gao",
    "year": 2025,
    "publication_date": "2025-12-04 18:58:18+00:00",
    "venue": "cs.CV",
    "publisher": "arXiv",
    "abstract": "In recent years, Image Quality Assessment (IQA) for AI-generated images (AIGI) has advanced rapidly; however, existing methods primarily target portraits and artistic images, lacking a systematic evaluation of interior scenes. We introduce Spatial Aesthetics, a paradigm that assesses the aesthetic quality of interior images along four dimensions: layout, harmony, lighting, and distortion. We construct SA-BENCH, the first benchmark for spatial aesthetics, comprising 18,000 images and 50,000 precise annotations. Employing SA-BENCH, we systematically evaluate current IQA methodologies and develop SA-IQA, through MLLM fine-tuning and a multidimensional fusion approach, as a comprehensive reward framework for assessing spatial aesthetics. We apply SA-IQA to two downstream tasks: (1) serving as a reward signal integrated with GRPO reinforcement learning to optimize the AIGC generation pipeline, and (2) Best-of-N selection to filter high-quality images and improve generation quality. Experiments indicate that SA-IQA significantly outperforms existing methods on SA-BENCH, setting a new standard for spatial aesthetics evaluation. Code and dataset will be open-sourced to advance research and applications in this domain.",
    "url": "http://arxiv.org/abs/2512.05098v1",
    "pdf_url": "https://arxiv.org/pdf/2512.05098v1",
    "citation_count": 0,
    "fetched_at": "2025-12-07 09:56:09.398056"
  },
  {
    "title": "Visual Reasoning Tracer: Object-Level Grounded Reasoning Benchmark",
    "paper_id": "2512.05091v1",
    "source": "arxiv",
    "authors": "Haobo Yuan, Yueyi Sun, Yanwei Li, Tao Zhang, Xueqing Deng, Henghui Ding, Lu Qi, Anran Wang, Xiangtai Li, Ming-Hsuan Yang",
    "first_author": "Haobo Yuan",
    "year": 2025,
    "publication_date": "2025-12-04 18:55:34+00:00",
    "venue": "cs.CV",
    "publisher": "arXiv",
    "abstract": "Recent advances in Multimodal Large Language Models (MLLMs) have significantly improved performance on tasks such as visual grounding and visual question answering. However, the reasoning processes of these models remain largely opaque; they typically output only final predictions without revealing the intermediate steps or fine-grained evidence (e.g., pixels, locations) that lead to the result. This contrasts with human intelligence, which naturally operates through a chain of visual reasoning. To address this limitation, we introduce the Visual Reasoning Tracer (VRT) task, which requires models to not only localize the target object but also explicitly predict the intermediate objects that form the reasoning path. To advance research in this area, we contribute: (1) VRT-Bench, a human-annotated benchmark for evaluating visual reasoning; (2) a new metric for assessing the quality of reasoning traces; and (3) VRT-80k, a large-scale dataset for reasoning model training. Our experiments reveal that while existing models often produce the correct final output, they struggle to ground their intermediate reasoning. In contrast, models trained on VRT-80k achieve substantial improvements in tracing the reasoning path.",
    "url": "http://arxiv.org/abs/2512.05091v1",
    "pdf_url": "https://arxiv.org/pdf/2512.05091v1",
    "citation_count": 0,
    "fetched_at": "2025-12-07 09:56:09.398088"
  },
  {
    "title": "The Geometry of Intelligence: Deterministic Functional Topology as a Foundation for Real-World Perception",
    "paper_id": "2512.05089v1",
    "source": "arxiv",
    "authors": "Eduardo Di Santi",
    "first_author": "Eduardo Di Santi",
    "year": 2025,
    "publication_date": "2025-12-04 18:54:07+00:00",
    "venue": "cs.LG",
    "publisher": "arXiv",
    "abstract": "Real-world physical processes do not generate arbitrary variability: their signals concentrate on compact and low-variability subsets of functional space. This geometric structure enables rapid generalization from a few examples in both biological and artificial systems.\n  This work develops a deterministic functional-topological framework in which the set of valid realizations of a physical phenomenon forms a compact perceptual manifold with stable invariants and a finite Hausdorff radius. We show that the boundaries of this manifold can be discovered in a fully self-supervised manner through Monte Carlo sampling, even when the governing equations of the system are unknown.\n  We provide theoretical guarantees, practical estimators of knowledge boundaries, and empirical validations across three domains: electromechanical railway point machines, electrochemical battery discharge curves, and physiological ECG signals.\n  Our results demonstrate that deterministic functional topology offers a unified mathematical foundation for perception, representation, and world-model construction, explaining why biological learners and self-supervised AI models can generalize from limited observations.",
    "url": "http://arxiv.org/abs/2512.05089v1",
    "pdf_url": "https://arxiv.org/pdf/2512.05089v1",
    "citation_count": 0,
    "fetched_at": "2025-12-07 09:56:09.398111"
  },
  {
    "title": "David vs. Goliath: Can Small Models Win Big with Agentic AI in Hardware Design?",
    "paper_id": "2512.05073v1",
    "source": "arxiv",
    "authors": "Shashwat Shankar, Subhranshu Pandey, Innocent Dengkhw Mochahari, Bhabesh Mali, Animesh Basak Chowdhury, Sukanta Bhattacharjee, Chandan Karfa",
    "first_author": "Shashwat Shankar",
    "year": 2025,
    "publication_date": "2025-12-04 18:37:29+00:00",
    "venue": "cs.LG",
    "publisher": "arXiv",
    "abstract": "Large Language Model(LLM) inference demands massive compute and energy, making domain-specific tasks expensive and unsustainable. As foundation models keep scaling, we ask: Is bigger always better for hardware design? Our work tests this by evaluating Small Language Models coupled with a curated agentic AI framework on NVIDIA's Comprehensive Verilog Design Problems(CVDP) benchmark. Results show that agentic workflows: through task decomposition, iterative feedback, and correction - not only unlock near-LLM performance at a fraction of the cost but also create learning opportunities for agents, paving the way for efficient, adaptive solutions in complex design tasks.",
    "url": "http://arxiv.org/abs/2512.05073v1",
    "pdf_url": "https://arxiv.org/pdf/2512.05073v1",
    "citation_count": 0,
    "fetched_at": "2025-12-07 09:56:09.398138"
  }
]