# Research Tracker - Technical Decisions & Implementation Plan

## Project Overview
An intelligent research assistant that fetches, analyzes, and summarizes the newest Google Scholar papers in AI and robotics. Designed to help investors stay ahead of technological trends and make data-driven investment decisions.

## Tech Stack Decisions

### 1. Paper Fetching & Data Collection
**Decision: scholarly (Python library)**
- âœ… FREE - No API costs
- âœ… Direct Google Scholar scraping
- âœ… Python-native, easy integration
- âš ï¸ Respectful rate limiting required
- ğŸ“¦ Backup: arXiv API for supplementary papers

**Keywords to track:**
- Artificial Intelligence
- Machine Learning
- Deep Learning
- Computer Vision
- Natural Language Processing
- Robotics
- Autonomous Systems
- Reinforcement Learning
- LLM / Large Language Models
- Generative AI

### 2. AI Summarization & Translation
**Decision: ç«å±±æ–¹èˆŸå¤§æ¨¡å‹ API (Volcano Engine / ByteDance)**
- âœ… Cost-effective for Chinese market
- âœ… Native Chinese language support
- âœ… Can handle both summarization + translation in one call
- âœ… Good for technical content
- ğŸ”„ **To be implemented after fetch process is complete**

**Alternative considered:**
- DeepSeek API (backup option)
- OpenAI GPT-4o-mini (if Volcano Engine doesn't work)

**Output Language:** Chinese (ä¸­æ–‡)
**Summary Focus:** Investment insights, technology trends, commercial viability

### 3. Publishing & Notifications
**Decision: Lark (Feishu/é£ä¹¦) Bot API**
- âœ… FREE webhook integration
- âœ… Rich message formatting (cards, markdown)
- âœ… Easy group notifications
- âœ… Better API than WeChat Official Account
- âœ… Supports images, links, interactive cards

**Message Format:**
- Daily digest at 9:00 AM
- Top 5-10 papers with summaries
- Trend highlights
- Investment signals

### 4. Database & Storage
**To be decided based on scale:**
- SQLite (initial development)
- PostgreSQL (production)
- Optional: Vector DB for semantic search later

### 5. Scheduling
**Options:**
- APScheduler (Python, simple)
- Cron job (Unix)
- GitHub Actions (cloud option)

## Implementation Phases

### Phase 1: Data Collection (Current Focus)
- [ ] Set up project structure
- [ ] Implement scholarly scraper
- [ ] Add keyword filtering
- [ ] Test daily paper fetching
- [ ] Store raw data (JSON/SQLite)

### Phase 2: AI Processing (After Phase 1)
- [ ] Integrate ç«å±±æ–¹èˆŸå¤§æ¨¡å‹ API
- [ ] Design prompts for investment-focused summaries
- [ ] Generate Chinese summaries
- [ ] Extract key insights

### Phase 3: Publishing
- [ ] Set up Lark Bot
- [ ] Design message templates
- [ ] Implement daily digest
- [ ] Add error handling

### Phase 4: Enhancement
- [ ] Add trend analysis
- [ ] Historical data comparison
- [ ] Citation tracking
- [ ] Web dashboard (optional)

## Development Notes

**Rate Limiting:**
- Google Scholar: Be respectful, add delays (2-5 seconds between requests)
- Implement exponential backoff on failures
- Cache results to avoid re-fetching

**Data Quality:**
- Filter by publication date (last 24-48 hours)
- Filter by citation count (optional quality threshold)
- Deduplicate papers
- Validate metadata

**Error Handling:**
- Retry logic for network failures
- Fallback to arXiv if Scholar fails
- Log all errors
- Send alerts on critical failures

## Environment Variables Needed

```env
# ç«å±±æ–¹èˆŸå¤§æ¨¡å‹ API (Phase 2)
VOLCANO_API_KEY=your_api_key_here
VOLCANO_API_ENDPOINT=https://ark.cn-beijing.volces.com/api/v3

# Lark Bot (Phase 3)
LARK_WEBHOOK_URL=your_webhook_url_here

# Optional
ARXIV_API_KEY=optional
SMTP_CONFIG=for_email_backup
```

## Dependencies (requirements.txt)

```txt
# Data Collection
scholarly>=1.7.11
arxiv>=2.0.0
requests>=2.31.0
beautifulsoup4>=4.12.0

# Data Processing
pandas>=2.1.0
python-dateutil>=2.8.2

# AI/LLM (Phase 2)
openai>=1.0.0  # For Volcano Engine SDK
httpx>=0.25.0

# Database
sqlalchemy>=2.0.0
sqlite3  # Built-in

# Scheduling
apscheduler>=3.10.0

# Utilities
python-dotenv>=1.0.0
pyyaml>=6.0

# Logging
loguru>=0.7.0
```

## Project Structure

```
research-tracker/
â”œâ”€â”€ copilot.md                    # This file
â”œâ”€â”€ README.md                     # Project overview
â”œâ”€â”€ requirements.txt              # Python dependencies
â”œâ”€â”€ .env.example                  # Environment template
â”œâ”€â”€ .gitignore                    # Git ignore rules
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ settings.py          # Configuration management
â”‚   â”œâ”€â”€ scrapers/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ base_scraper.py      # Base scraper class
â”‚   â”‚   â”œâ”€â”€ scholar_scraper.py   # Google Scholar with scholarly
â”‚   â”‚   â””â”€â”€ arxiv_scraper.py     # arXiv backup
â”‚   â”œâ”€â”€ processors/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ summarizer.py        # Volcano Engine summarization
â”‚   â”‚   â””â”€â”€ translator.py        # If needed separately
â”‚   â”œâ”€â”€ publishers/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ lark_bot.py          # Lark/Feishu notifications
â”‚   â”œâ”€â”€ database/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ models.py            # Data models
â”‚   â”‚   â””â”€â”€ repository.py        # Database operations
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ logger.py            # Logging setup
â”‚       â””â”€â”€ helpers.py           # Utility functions
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ daily_fetch.py           # Main daily job
â”‚   â””â”€â”€ test_scraper.py          # Testing scripts
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ papers.db                # SQLite database
â”‚   â””â”€â”€ logs/                    # Log files
â””â”€â”€ tests/
    â”œâ”€â”€ __init__.py
    â””â”€â”€ test_scraper.py          # Unit tests
```

## Next Steps

1. âœ… Create project structure
2. âœ… Set up Python virtual environment
3. â³ Implement scholarly scraper (Phase 1)
4. â³ Test with real queries
5. â³ Add data storage
6. ğŸ”„ Integrate Volcano Engine API (Phase 2)
7. ğŸ”„ Set up Lark Bot (Phase 3)

---

**Last Updated:** December 7, 2025  
**Status:** Planning & Initial Setup  
**Current Phase:** Phase 1 - Data Collection
